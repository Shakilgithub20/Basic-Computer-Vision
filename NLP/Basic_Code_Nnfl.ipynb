{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic_Code_Nnfl.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt8_SxJDEROo"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JatwTBNEKa1Z"
      },
      "source": [
        "def sigmoid(z, derive=False):\n",
        "  if derive == True:\n",
        "    return z*(1-z)\n",
        "    \n",
        "  # sigmoid rule\n",
        "  return 1/(1+np.exp(-z))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4sh4vz7EovG",
        "outputId": "49ffda2d-ffaf-4f47-f5fd-2a4ed25363a4"
      },
      "source": [
        "# Training Set\n",
        "X = np.array( [[1, 0, 0, 0],\n",
        "               [1, 0, 0, 1],\n",
        "               [1, 0, 1, 0],\n",
        "               [1, 1, 0, 1],\n",
        "               [1, 1, 0, 0],\n",
        "               [1, 1, 1, 1]] )\n",
        "\n",
        "# Gold/Actual Label or Output\n",
        "y = np.array([[0],\n",
        "              [0],\n",
        "              [0],\n",
        "              [0],\n",
        "              [0],\n",
        "              [1]])\n",
        "\n",
        "# Initialize the wights - randomly or with zeroes\n",
        "# w = np.random.random((4, 1))\n",
        "w = np.zeros((4, 1))\n",
        "w"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlXrI8nMIQZP"
      },
      "source": [
        "# Training the ANN (perceptron)... Setting No. of Iteration (epochs)\n",
        "for iter in range(1000):\n",
        "  # Forward Propagation: to Predict output (y_hat)\n",
        "  z = np.dot(X, w)\n",
        "  y_hat = sigmoid(z)\n",
        "  # print(y_hat)\n",
        "\n",
        "  # simple 'cost' calculation\n",
        "  # cost = pow((y - y_hat), 2) * 0.5    # M.S.E = Mean Squared Error\n",
        "  # cost = abs(y - y_hat) * 0.5\n",
        "  cost = y - y_hat\n",
        "\n",
        "  # Back-propagtion (future studies)\n",
        "  dy_hat = sigmoid(y_hat, derive=True)\n",
        "  delta_w = cost * dy_hat\n",
        "\n",
        "  # update weights\n",
        "  w += np.dot(X.T, delta_w)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdQlAZHLOY94",
        "outputId": "76781554-beb9-47d7-c87f-a12f8b56f8af"
      },
      "source": [
        "print(\"\\n Output after Training ...\")\n",
        "print(y_hat)\n",
        "\n",
        "print(\"\\n Weights:\")\n",
        "print(w)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Output after Training ...\n",
            "[[1.85293178e-04]\n",
            " [3.24981421e-03]\n",
            " [4.84029659e-02]\n",
            " [5.42477690e-02]\n",
            " [3.24981421e-03]\n",
            " [9.40273030e-01]]\n",
            "\n",
            " Weights:\n",
            "[[-8.5950654 ]\n",
            " [ 2.86804413]\n",
            " [ 5.61593016]\n",
            " [ 2.86804413]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO8nZja-P1xt",
        "outputId": "59bbd44f-fcca-40aa-958f-9f3e4e4eacba"
      },
      "source": [
        "# Test Set\n",
        "T = np.array([[1, 0, 1, 1],\n",
        "              [1, 1, 1, 0]])\n",
        "\n",
        "# Final Predictions on the Test Set\n",
        "y_hat = sigmoid(np.dot(T, w))\n",
        "\n",
        "print(\"\\nTest Predictions...\")\n",
        "print(y_hat)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Predictions...\n",
            "[[0.47225575]\n",
            " [0.47225575]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7n7iBL9Ryp1H"
      },
      "source": [
        ""
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdW8fzpanZTr"
      },
      "source": [
        "sentence_bn = ['আমরা আজ ল্যাব ক্লাস এ নিউরাল নেটওয়ার্ক প্রয়োগ করব', 'আমাদের প্রোগ্রামিং অনেক ভালো লাগে', 'আমরা সবাই এই কোর্সে ভালো মার্ক পেতে চাই']\n",
        "sentence_en = ['We do not know how to implement a neural network', 'We don\\'t like programking at all', 'We are in great doubt about our marks in this course']"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3yP1_CeosWd",
        "outputId": "3a27d49c-7928-4b0b-f4ff-bf312c50d3cf"
      },
      "source": [
        "import nltk\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diXsU2vrpCug",
        "outputId": "e83e672c-2474-4415-adfd-175fd1a12325"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vec = CountVectorizer()\n",
        "# vec.fit(sentence_bn)\n",
        "model = vec.fit_transform(sentence_en)\n",
        "\n",
        "vec.vocabulary_\n",
        "vocab_list = vec.get_feature_names()\n",
        "print(vocab_list)\n",
        "# print(model.toarray())\n",
        "count_list = model.toarray().sum(axis=0)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['about', 'all', 'are', 'at', 'course', 'do', 'don', 'doubt', 'great', 'how', 'implement', 'in', 'know', 'like', 'marks', 'network', 'neural', 'not', 'our', 'programking', 'this', 'to', 'we']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kLWD_cYvWYk",
        "outputId": "23e146b4-3635-4ba8-9b75-ba08156a7abb"
      },
      "source": [
        "print(dict(zip(vocab_list,count_list)))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'about': 1, 'all': 1, 'are': 1, 'at': 1, 'course': 1, 'do': 1, 'don': 1, 'doubt': 1, 'great': 1, 'how': 1, 'implement': 1, 'in': 2, 'know': 1, 'like': 1, 'marks': 1, 'network': 1, 'neural': 1, 'not': 1, 'our': 1, 'programking': 1, 'this': 1, 'to': 1, 'we': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPLjz4YCvu6Z",
        "outputId": "0bf347de-dfe4-4cea-ca43-cb68ce717301"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "vec_new = CountVectorizer(encoding='utf-8', tokenizer=word_tokenize)\n",
        "vec_new.fit(sentence_bn)\n",
        "\n",
        "vocab_list_bn = vec_new.get_feature_names()\n",
        "print(vocab_list_bn)\n",
        "print(vec_new.vocabulary_)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['অনেক', 'আজ', 'আমরা', 'আমাদের', 'এ', 'এই', 'করব', 'কোর্সে', 'ক্লাস', 'চাই', 'নিউরাল', 'নেটওয়ার্ক', 'পেতে', 'প্রোগ্রামিং', 'প্রয়োগ', 'ভালো', 'মার্ক', 'লাগে', 'ল্যাব', 'সবাই']\n",
            "{'আমরা': 2, 'আজ': 1, 'ল্যাব': 18, 'ক্লাস': 8, 'এ': 4, 'নিউরাল': 10, 'নেটওয়ার্ক': 11, 'প্রয়োগ': 14, 'করব': 6, 'আমাদের': 3, 'প্রোগ্রামিং': 13, 'অনেক': 0, 'ভালো': 15, 'লাগে': 17, 'সবাই': 19, 'এই': 5, 'কোর্সে': 7, 'মার্ক': 16, 'পেতে': 12, 'চাই': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aO98z51Hxeu1",
        "outputId": "5aac2453-288b-4660-a639-14d5d82e2fbd"
      },
      "source": [
        "# Tokenization\n",
        "\n",
        "tokenized_bn = []\n",
        "\n",
        "new_sentence = sentence_bn + ['এইখানে আমরা নতুন আরো একটি বাক্য রাখলাম']\n",
        "\n",
        "for sent in new_sentence:\n",
        "  tokenized_sent = word_tokenize(sent)\n",
        "  # print(tokenized_sent)\n",
        "  tokenized_bn.append(tokenized_sent)\n",
        "\n",
        "print(tokenized_bn)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['আমরা', 'আজ', 'ল্যাব', 'ক্লাস', 'এ', 'নিউরাল', 'নেটওয়ার্ক', 'প্রয়োগ', 'করব'], ['আমাদের', 'প্রোগ্রামিং', 'অনেক', 'ভালো', 'লাগে'], ['আমরা', 'সবাই', 'এই', 'কোর্সে', 'ভালো', 'মার্ক', 'পেতে', 'চাই'], ['এইখানে', 'আমরা', 'নতুন', 'আরো', 'একটি', 'বাক্য', 'রাখলাম']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fgLAzMixBTW"
      },
      "source": [
        "# Manually Implement the Number of Occurences of each words in the sentence_bn dataset - C.W"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXBzX-7-1J9a",
        "outputId": "b62979ce-6d11-48b3-b134-7249fa62132f"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "std_names = ['জিয়াদ', 'নয়ন', 'কিশোর', 'বিলাস', 'ইমতিয়াজুল', 'মনিরুল' , 'নয়ন']\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "name_labels = encoder.fit_transform(std_names)\n",
        "\n",
        "# dense representation\n",
        "print(name_labels)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 3 1 4 0 5 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsBp-m1o1Ju0",
        "outputId": "0fb8d78d-1dc7-48d5-8d7f-5ad0b3df8949"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder()\n",
        "name_labels = name_labels.reshape((7, 1))\n",
        "\n",
        "sparse_rep = encoder.fit_transform(name_labels).toarray()\n",
        "print(sparse_rep)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ErNQ4BHy-0-"
      },
      "source": [
        ""
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4pk2muJSvYz",
        "outputId": "4e4487e8-5136-4b43-ef89-b628389ec368"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbHHVXbKXc4z"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "embedding_layer = layers.Embedding(1000, 5)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpWAb8YXX44N",
        "outputId": "6417ca7b-3326-4b32-fd45-5713cf1a2546"
      },
      "source": [
        "result = embedding_layer(tf.constant([1, 2, 3]))\n",
        "result.numpy()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.04963868, -0.01968982,  0.01111265, -0.02539588,  0.0450342 ],\n",
              "       [-0.02905295, -0.03385232,  0.02321938, -0.0279748 , -0.00261045],\n",
              "       [ 0.02179753, -0.0216635 ,  0.00325959,  0.03234151,  0.01134565]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEzJFRzbYZ71",
        "outputId": "404befad-2228-48b5-ba7a-ae89b74b7a58"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "sentence = ['এই মাত্র পাওয়া সংবাদে জানা গেলো দেশ এর করোনা পরিস্থিতির উন্নতি হয়েছে', \n",
        "            'আমাদের সমাজে মুখোশধারী মানুষের অভাব নাই', \n",
        "            'আমরা দিন দিন বোকার রাজ্যে নির্বাসিত হচ্ছি']\n",
        "\n",
        "tokenizer.fit_on_texts(sentence)\n",
        "\n",
        "sequence = tokenizer.texts_to_sequences(sentence)\n",
        "\n",
        "sequence"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13],\n",
              " [14, 15, 16, 17, 18, 19],\n",
              " [20, 1, 1, 21, 22, 23, 24]]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G10BENsTafAa",
        "outputId": "acc22cba-dbde-468f-9870-2f39f043acc4"
      },
      "source": [
        "result = embedding_layer(tf.constant(sequence[0]))\n",
        "result.numpy()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.02905295, -0.03385232,  0.02321938, -0.0279748 , -0.00261045],\n",
              "       [ 0.02179753, -0.0216635 ,  0.00325959,  0.03234151,  0.01134565],\n",
              "       [ 0.03221888,  0.04403212,  0.04695613,  0.04018353, -0.00687975],\n",
              "       [ 0.04763016,  0.0004434 , -0.02749204,  0.02341368, -0.03677921],\n",
              "       [-0.02460402, -0.03339466,  0.00023228,  0.02098242,  0.00367998],\n",
              "       [ 0.02211041, -0.02584176, -0.0296883 ,  0.04999756, -0.03297983],\n",
              "       [ 0.04393085,  0.0045329 ,  0.01364804, -0.03426585, -0.02552226],\n",
              "       [-0.00521391,  0.0494909 ,  0.00989445,  0.00033556, -0.02501909],\n",
              "       [-0.01824832, -0.04194999, -0.01358656, -0.04907768,  0.02796395],\n",
              "       [-0.000151  ,  0.02046824,  0.03847941,  0.02208743,  0.01204553],\n",
              "       [ 0.02858442,  0.00445946, -0.03480124,  0.0341277 , -0.00523623],\n",
              "       [-0.01782662,  0.01902096,  0.03889645,  0.00678461,  0.00323581]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF357AqLhRHO"
      },
      "source": [
        "from numpy import array\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Embedding, Dense, LSTM, Bidirectional"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cICBB2hTh5r7"
      },
      "source": [
        "# Real Life Example of Classification\n",
        "\n",
        "train_ex = ['পণ্য ১০০% অরজিনাল কিন্তু আমার সাইজ যেটা আসছে ওটা আমাকে হচ্ছে না। আমার দরকার ৪২',\n",
        "             'জুতা এপেক্সের ছিল একটু ভারী মনে হয়েছে জুতা এবং শক্ত। প্রোডাক্টটি ঠিক আছে যা চেয়েছিলাম তাই পেয়েছি  ওভারঅল ভালো',\n",
        "             'আমি বিস্মিত, ঠিক যেমনটি চেয়েছিলাম তেমনটি পেয়েছি।। ধন্যবাদ এপেক্স ধন্যবাদ দারাজ।।',\n",
        "             'অসাধারণ...ধন্যবাদ দারাজ।ধন্যবাদ এপেক্স। অরিজিনাল প্রোডাক্ট দেওয়ার জন্য।',\n",
        "             'বেশি বলবনা এককথায় একশতে একশ। দাম অনুযায়ী খুবইসুন্দর প্রোডাক্ট, ধন্যবাদ দারাজ এবং সেলার ভাইটিকে।',\n",
        "             'খুব একটা ভালো বলা চলে না। চাইলাম ৪১ আর দিলো ৪০।।ওনারা নিজেরাই ভালো রিভিউ দেয় কাস্টমারদের দেখানোর জন্ন্যে',\n",
        "             'হাটার সময় অনেক আন ইজি পা বাকাতে প্রব্লেম হয়',\n",
        "             'এপেক্স এর মত এই রকম প্রোডাক্ট আশা করা যায় না',\n",
        "             'এপেক্স তো সবসময়ই ভালো বাট ডেলিভারি বাজে ছিলো😡😡😡 যেদিন দেয়ার কথা এর ২ দিন পর দিছে...',\n",
        "             'ফালতু সেলার। মেসেজ দিয়া বল্লাম সাইজ যাতে উল্টোপাল্টা না আসে। কেউ অরডার করে প্রতারিত হবেন না।।'\n",
        "             ]\n",
        "\n",
        "# Reviews -- negative = 0 || positive = 1 (class/labels)\n",
        "train_label = array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iBFjv-kbkdLB",
        "outputId": "3f12d0cd-d1c9-4195-9c9f-d8396402cbc7"
      },
      "source": [
        "train_ex[3]"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'অসাধারণ...ধন্যবাদ দারাজ।ধন্যবাদ এপেক্স। অরিজিনাল প্রোডাক্ট দেওয়ার জন্য।'"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OWv_Zwjkjy-",
        "outputId": "5f77b1c5-ac35-439c-e145-772c95a44f18"
      },
      "source": [
        "# tokenization and converting words into sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_ex)\n",
        "dense_train_ex = tokenizer.texts_to_sequences(train_ex)\n",
        "\n",
        "dense_train_ex"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[14, 15, 16, 17, 5, 6, 18, 19, 20, 21, 22, 7, 5, 23, 24],\n",
              " [8, 25, 26, 27, 28, 29, 30, 8, 9, 31, 32, 10, 33, 34, 11, 35, 36, 37, 1],\n",
              " [38, 39, 10, 40, 11, 41, 42, 2, 3, 2, 43],\n",
              " [44, 2, 45, 46, 47, 4, 48, 49],\n",
              " [50, 51, 52, 53, 54, 55, 56, 57, 4, 2, 58, 9, 59, 60],\n",
              " [61, 62, 1, 63, 64, 7, 65, 66, 67, 68, 69, 70, 1, 71, 72, 73, 74, 75],\n",
              " [76, 77, 78, 79, 80, 81, 82, 83, 84],\n",
              " [3, 12, 85, 86, 87, 4, 88, 89, 90, 13],\n",
              " [3, 91, 92, 1, 93, 94, 95, 96, 97, 98, 99, 12, 100, 101, 102, 103],\n",
              " [104, 105, 106, 107, 108, 6, 109, 110, 13, 111, 112, 113, 114, 115, 116, 117]]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AgdiZbTnIfZ",
        "outputId": "86a54f7a-52ca-4643-f734-49eae14be1d0"
      },
      "source": [
        "# padding the training documents in order to make them equal length\n",
        "MAX_LENGTH = 16\n",
        "\n",
        "padded_train_ex = pad_sequences(dense_train_ex, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "for pd_sen in padded_train_ex:\n",
        "  print(pd_sen)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14 15 16 17  5  6 18 19 20 21 22  7  5 23 24  0]\n",
            "[27 28 29 30  8  9 31 32 10 33 34 11 35 36 37  1]\n",
            "[38 39 10 40 11 41 42  2  3  2 43  0  0  0  0  0]\n",
            "[44  2 45 46 47  4 48 49  0  0  0  0  0  0  0  0]\n",
            "[50 51 52 53 54 55 56 57  4  2 58  9 59 60  0  0]\n",
            "[ 1 63 64  7 65 66 67 68 69 70  1 71 72 73 74 75]\n",
            "[76 77 78 79 80 81 82 83 84  0  0  0  0  0  0  0]\n",
            "[ 3 12 85 86 87  4 88 89 90 13  0  0  0  0  0  0]\n",
            "[  3  91  92   1  93  94  95  96  97  98  99  12 100 101 102 103]\n",
            "[104 105 106 107 108   6 109 110  13 111 112 113 114 115 116 117]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hu9NqNL9n3Zb",
        "outputId": "a2761ba0-b481-4e79-a4c4-748851db0572"
      },
      "source": [
        "# Model Declaration\n",
        "VOCAB_SIZE = 118\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Embedding Layer\n",
        "embedding_layer = Embedding(input_dim=VOCAB_SIZE, output_dim=8, input_length=MAX_LENGTH)\n",
        "model.add(embedding_layer)\n",
        "\n",
        "# # Flatten Layer\n",
        "# model.add(Flatten())\n",
        "\n",
        "# model.add(Dense(units=160, activation='relu'))\n",
        "# model.add(Dense(units=80, activation='relu'))\n",
        "# model.add(Dense(units=40, activation='relu'))\n",
        "# model.add(Dense(units=10, activation='relu'))\n",
        "\n",
        "# LSTM - for better performance\n",
        "# model.add(LSTM(units=128))\n",
        "\n",
        "# Bidirectional LSTM\n",
        "forward_layers = LSTM(units=128, return_sequences=False)\n",
        "backward_layers = LSTM(units=128, return_sequences=False, go_backwards=True)\n",
        "model.add(Bidirectional(layer=forward_layers, backward_layer=backward_layers))\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['acc'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 16, 8)             944       \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 256)               140288    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 141,489\n",
            "Trainable params: 141,489\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InvCtAjsrxPX",
        "outputId": "9cf82246-afe8-48b8-ae96-a46ab1736b14"
      },
      "source": [
        "model.fit(padded_train_ex, train_label, epochs=10, verbose=1)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2500 - acc: 0.4000\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.2496 - acc: 0.5000\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.2492 - acc: 0.7000\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.2487 - acc: 0.7000\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2481 - acc: 0.7000\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.2473 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.2464 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.2452 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.2438 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.2419 - acc: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9e574a5b50>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBAB4FT4sKO9",
        "outputId": "ae832bbf-1ea2-42a4-adcb-242c7be4cae7"
      },
      "source": [
        "# Testing\n",
        "test_ex = ['দামে বেশি হলেও মানে ভালো, ধন্যবাদ এপেক্স এবং দারাজ কে', \n",
        "           'জুতাটি হাতে পেয়ে আমি সত্যিই বিস্মিত', \n",
        "           'একদম বাজে, মনে হচ্ছে প্রতারিত হলাম 😡',\n",
        "           'এতো ফালতু প্রডাক্ট পবো আশা করি নি']\n",
        "\n",
        "# tokenization and converting words into sequence\n",
        "dense_test_ex = tokenizer.texts_to_sequences(test_ex)\n",
        "\n",
        "# padding the test documents\n",
        "padded_test_ex = pad_sequences(dense_test_ex, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "prediction = model.predict(padded_test_ex)\n",
        "\n",
        "print(prediction)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.501005  ]\n",
            " [0.5013156 ]\n",
            " [0.50083774]\n",
            " [0.49560586]]\n"
          ]
        }
      ]
    }
  ]
}